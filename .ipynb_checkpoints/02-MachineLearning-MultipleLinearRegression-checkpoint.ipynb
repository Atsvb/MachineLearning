{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Multiple Linear Regression</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider now multiple features from our sample set. So we will work with a feature matrix $X$ of shape $(m,D)$ where\n",
    "$m$ is the number of samples and $D$ the number of features. Our label vector $Y$ is of length $m$. \n",
    "Our aim is to determine a vector of wights $W$ of length $D+1$ ($D$ features plus the intercept)\n",
    "The estimation for a given set of features is:\n",
    "$${\\hat y}= w_0+\\sum_1^Dx_iw_i$$\n",
    "\n",
    "To vectorize it is more convenient to add a column of $1$'s as first column of the feature matrix, so the estimation vector for a whole sample set will be:\n",
    "$${\\hat Y}=X \\cdot W$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the output given W, and the feature matrix, the vector of weights, W[0] is the intercept\n",
    "def predict_output(X, W):\n",
    "    pred=np.matmul(X,W)\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the cost we will use a similar formula to the one we used for simple linear regression:\n",
    "\n",
    "$$cost=\\frac{1}{2m}\\sum({\\hat y_i}-y_i)^2$$\n",
    "the only difference is that we divide by $2m$ instead of dividing by $m$. This makes the computation simpler.\n",
    "In verctor form the cost is:\n",
    "\n",
    "$$cost=\\frac{1}{2m}({\\hat Y -Y})^t \\cdot({\\hat Y -Y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute cost\n",
    "def get_cost(X, Y, W):\n",
    "    #X is the feature vector (m,D+1)\n",
    "    #Y is the labels vector (m,1)\n",
    "    m=X.shape[0]\n",
    "    pred=predict_output(X, W)\n",
    "    res=pred-Y\n",
    "    sqrd=res**2\n",
    "    cost=np.sum(sqrd)/(2*m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an exact formula for multiple regression, if we calculate the gradient of the cost with respect to $W$ and solve this gradient =0 we get:\n",
    "\n",
    "$$W=(X^tX)^{-1}X^tY$$\n",
    "\n",
    "However we cannot assure that the matrix $X^tX$ is invertible, and even if it is, the computational cost to find $W$ this way can be high. Instead we better use gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the cost with respect to $W$ is:\n",
    "\n",
    "$$\\nabla cost =  \\frac{-1}{m}X^t (Y-XW)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,Y,W):\n",
    "    m=X.shape[0]\n",
    "    preds=predict_output(X, W)\n",
    "    #print(preds)\n",
    "    error=Y-preds\n",
    "    #print(X.shape)\n",
    "    #print(error)\n",
    "    grad=-np.matmul(X.T,error)/m\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights update is the same as the case for simple regression:\n",
    "\n",
    "$$W=W- \\eta \\nabla cost$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_mult(X, Y, learning_rate, tolerance, initial_weights,max_iter=5000):\n",
    "    #X is the feature matrix\n",
    "    #Y is the output vector\n",
    "    #print(initial_weights.shape)\n",
    "\n",
    "    W=initial_weights\n",
    "    #print(initial_weights)\n",
    "    #print(W)\n",
    "    converged = False\n",
    "    k=0\n",
    "    while k<max_iter and not converged:\n",
    "        if k % 1000 == 0:\n",
    "            print(\"Iteration: \"+str(k))\n",
    "        #print(W)\n",
    "        #preds=predict_output(X, W)\n",
    "        #print(preds)\n",
    "        #error=Y-preds\n",
    "        #print(X.shape)\n",
    "        #print(error)\n",
    "        #grad=-2*np.matmul(X.T,error)\n",
    "        grad=gradient(X,Y,W)\n",
    "        W=W-learning_rate*grad\n",
    "        #W=W+2*learning_rate*np.matmul(X.T,error)\n",
    "        grad_norm=np.linalg.norm(grad)\n",
    "        #print(grad)\n",
    "        #print(grad_norm)\n",
    "        k=k+1\n",
    "        if grad_norm < tolerance:\n",
    "            converged= True\n",
    "            print(\"Converged at iteration \"+str(k))\n",
    "   \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load  our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices=pd.read_csv(\"houseprices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will work now with multiple features, it will be useful to convert the set of our features of interest to a numpy matrix. As pointed above, it is more convenient to add a column of 1's to the matrix and so the intercept b will be part of the weights vector $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a list of features convert the corresponding dataframe to a numpy matrix\n",
    "def get_numpy_matrix(dataframe, features, output):\n",
    "    dftemp=dataframe.copy()\n",
    "    dftemp[\"ones\"]=1. #add a column of 1s to the dataframe\n",
    "    features=[\"ones\"]+features #add the ones name to the start of the features list\n",
    "    feat_dataframe=dftemp[features]\n",
    "    X=feat_dataframe.values\n",
    "    Y=dataframe[output].values\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our data set onto training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataframe into train and test sets\n",
    "def trainset__testset_split(df, train_percent=.8, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    m = len(df.index)\n",
    "    shuffle = np.random.permutation(df.index)\n",
    "\n",
    "    train_end = int(train_percent * m)\n",
    "    #validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.loc[shuffle[:train_end]] #use loc instead of ix\n",
    "    #validate = df.ix[perm[train_end:validate_end]]\n",
    "    test = df.loc[shuffle[train_end:]]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set= trainset__testset_split(house_prices, train_percent=.8, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>One Feature</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to apply our algorithm, intended for multiple features, to a single feature to see if it does the same as our simple feature algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y=get_numpy_matrix(house_prices, [\"LivingArea\"],\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['LivingArea']\n",
    "output = 'price'\n",
    "(X_1, Y_1) = get_numpy_matrix(train_set, features, output)\n",
    "initial_weights = np.array([15000., 10.])\n",
    "learning_rate = 1.e-10\n",
    "tolerance = 2.e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_1=gradient_descent_mult(X_1, Y_1, learning_rate, tolerance, initial_weights, 60000)\n",
    "print(\"The weight vector is: \"+str(weights_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the values are practically the same that we obtained with our simple feature gradient descent, however this new algorithm takes more iterations (but the iterations takes less time). This is due to the slight change we did on the cost function (divide by $2m$ instead of $m$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_train_1 = get_cost(X_1,Y_1, weights_1)\n",
    "print(np.format_float_scientific(cost_train_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_test_1, Y_test_1) = get_numpy_matrix(test_set, features, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the cost on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_test_1 = get_cost(X_test_1 ,Y_test_1, weights_1)\n",
    "print(np.format_float_scientific(cost_test_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider now an additional feature, \"LotSize\". And apply gradient descent to two features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Two Feature</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2 = ['LivingArea', 'LotSize'] # sqft_living15 is the average squarefeet for the nearest 15 neighbors. \n",
    "output = 'price'\n",
    "(X_2, Y_2) = get_numpy_matrix(train_set, features_2, output)\n",
    "initial_weights = np.array([15000., 10., 10.])\n",
    "learning_Rate = 1.e-10\n",
    "tolerance = 1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_2=gradient_descent_mult(X_2, Y_2, learning_rate, tolerance, initial_weights, 100000)\n",
    "print(\"The weight vector is: \"+str(weights_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_train_2 = get_cost(X_2,Y_2, weights_2)\n",
    "print(np.format_float_scientific(cost_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#difference between costs for the cases with one and two features.\n",
    "print(np.format_float_scientific(cost_train_1-cost_train_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_test_2, Y_test_2) = get_numpy_matrix(test_set, features_2, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_test_2 = get_cost(X_test_2,Y_test_2, weights_2)\n",
    "print(np.format_float_scientific(cost_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.format_float_scientific(cost_test_1- cost_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot all this using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "#This sets up the size of the figures, a bit larger than default.\n",
    "fig_size[0] = 12\n",
    "fig_size[1] = 9\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the function determined by the weights obtained (it is a plane)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return weights_2[0]+ weights_2[1]*x+weights_2[2]*y\n",
    "    #return np.sin(np.sqrt(x ** 2 + y ** 2))\n",
    "\n",
    "x = np.linspace(0, 3000, 30)\n",
    "y = np.linspace(-6, 15, 30)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the plot of the training set and the aproximations determined by the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "ax.set_xlabel('x_1')\n",
    "ax.set_ylabel('x_2')\n",
    "ax.set_zlabel('')\n",
    "#zdata = 15 * np.random.random(100)\n",
    "#xdata = np.sin(zdata) + 0.1 * np.random.randn(100)\n",
    "#ydata = np.cos(zdata) + 0.1 * np.random.randn(100)\n",
    "ax.scatter3D(X_2[:,1], X_2[:,2], Y_2, cmap='Greens')\n",
    "ax.view_init(10, 60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot this with different view points using ax.view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "ax.set_xlabel('x_1')\n",
    "ax.set_ylabel('x_2')\n",
    "ax.set_zlabel('')\n",
    "#zdata = 15 * np.random.random(100)\n",
    "#xdata = np.sin(zdata) + 0.1 * np.random.randn(100)\n",
    "#ydata = np.cos(zdata) + 0.1 * np.random.randn(100)\n",
    "ax.scatter3D(X_2[:,1], X_2[:,2], Y_2, cmap='Greens')\n",
    "ax.view_init(0, 90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Three Feature</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add a third feature and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3 = ['LivingArea', 'LotSize','Bedrooms'] # sqft_living15 is the average squarefeet for the nearest 15 neighbors. \n",
    "output = 'price'\n",
    "(X_3, Y_3) = get_numpy_matrix(train_set, features_3, output)\n",
    "initial_weights = np.array([15000., 10., 10.,10.])\n",
    "learning_Rate = 1.e-10\n",
    "tolerance = 1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_3=gradient_descent_mult(X_3, Y_3, learning_rate, tolerance, initial_weights, 100000)\n",
    "print(\"The weight vector is: \"+str(weights_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_train_3 = get_cost(X_3,Y_3, weights_3)\n",
    "print(np.format_float_scientific(cost_train_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_test_3, Y_test_3) = get_numpy_matrix(test_set, features_3, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_test_3 = get_cost(X_test_3,Y_test_3, weights_3)\n",
    "print(np.format_float_scientific(cost_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.format_float_scientific(cost_test_3- cost_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
